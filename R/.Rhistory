exc_len = c()
for(i in 1:length(v)){
min_proj = pmin(data_exp[,1]/v[i],data_exp[,2]/(1-v[i]))
thresh = quantile(min_proj,q)
exc_len[i] = length(min_proj[min_proj > thresh])
t[(length(t)+1):(length(t)+exc_len[i])] = min_proj[min_proj > thresh] - thresh
}
results = tryCatch(optim(par=par_init,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="BFGS",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output = results
} else {
optim_output = optim(par=par_init,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="Nelder-Mead",control = list(maxit=100000))
}
results = tryCatch(optim(par=optim_output$par,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="Nelder-Mead",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output2 = results
} else {
optim_output2 = optim(par=optim_output$par,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="BFGS",control = list(maxit=100000))
}
while(abs(optim_output2$val - optim_output$val)>=tol){
optim_output = optim_output2
results = tryCatch(optim(par=optim_output$par,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="Nelder-Mead",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output2 = results
} else {
optim_output2 = optim(par=optim_output$par,fn=beta_estimation3,basis=basis,t=t,exc_len=exc_len,lam_end=lam_end,v=v,method="BFGS",control = list(maxit=100000))
}
}
return(c(lam_end[1],exp(optim_output2$par),lam_end[2]))
}
beta_estimation4 = function(par,basis,frac,r,no_quant,lam_end,v){
beta = c(lam_end[1],exp(par),lam_end[2])
lam = basis%*%beta
if(sum(is.infinite(beta))>0 | sum(is.na(beta))>0){
return(1e10)
} else {
lambda_sum = sum(abs(frac-exp(-r*rep(lam,each=no_quant))))
return(lambda_sum)
}
}
minimisation_function4 = function(v,k,frac,r,no_quant,lam_end){
basis = bp2d_const(v=v,k=k,a=min(v),b=max(v))
tol = 0.0001
par_init = rep(0,k-1)
results = tryCatch(optim(par=par_init,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="BFGS",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output = results
} else {
optim_output = optim(par=par_init,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="Nelder-Mead",control = list(maxit=100000))
}
results = tryCatch(optim(par=optim_output$par,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="Nelder-Mead",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output2 = results
} else {
optim_output2 = optim(par=optim_output$par,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="BFGS",control = list(maxit=100000))
}
while(abs(optim_output2$val - optim_output$val)>=tol){
optim_output = optim_output2
results = tryCatch(optim(par=optim_output$par,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="Nelder-Mead",control = list(maxit=100000)),error = function(e){1})
if(is.list(results)){
optim_output2 = results
} else {
optim_output2 = optim(par=optim_output$par,fn=beta_estimation4,basis=basis,frac=frac,r=r,lam_end=lam_end,v=v,no_quant=no_quant,method="BFGS",control = list(maxit=100000))
}
}
return(c(lam_end[1],exp(optim_output2$par),lam_end[2]))
}
consistentEstimation <- function(data, quant=0.999, thresh.quant=0.5, nei=100, len=199, knot.num=7, deltas=seq(0.1,1,0.1), omegas=seq(0.1,1,0.1)){
# Rank transform to exponential margins:
data.E <- data
# Calculate pseudo-polar coordinates:
R.E <- apply(data.E,1,sum)
W.E <- data.E[,1]/R.E
# Take log(R) for threshold calculations:
logR.E <- log(R.E)
# Vector of angles W* at which to estimate the boundary:
Wstar <- unique(sort(c(quantile(W.E, seq(0,1,length.out=(len-1))),0.5)))
# For each value of W*: calculate the largest angular distance (from W*) in each neighbourhood;
# calculate estimates of the radial quantiles from a standard GPD fit with empirical quantile threshold:
thresh.move <- NULL
R.W.quant.ind <- NULL
for(sec.num in 1:length(Wstar)){
sec.W <- Wstar[sec.num]
eps.W <- sort(abs(W.E - sec.W))[nei]
nei.W <- which(abs(W.E - sec.W) <= eps.W)
thresh.move[sec.num] <- quantile(R.E[nei.W],thresh.quant)
fit <- ismev::gpd.fit(R.E[nei.W], threshold=thresh.move[sec.num], npy=length(nei.W),show = F)
R.W.quant.ind[sec.num] <- evd::qgpd((quant-thresh.quant)/(1-thresh.quant), loc=thresh.move[sec.num], scale=fit$mle[1], shape=fit$mle[2])
}
# Specify placement of knots for splines:
knots <- c(min(W.E), (min(W.E) + (max(W.E)-min(W.E))*c(1:(knot.num-2))/(knot.num-1)), max(W.E))
knots[(knot.num+1)/2] <- 0.5
knots1 <- list(W.E = c(0,knots,1)) # Linear spline knots
knots2 <- list(W.E = c(0,0,knots,1,1)) # Quadratic spline knots
knots3 <- list(W.E = c(0,0,0,knots,1,1,1)) # Cubic spline knots
# Use asymmetric Laplace for quantile regression of threshold:
# Fit on log(R) then back-transform:
fmla_ald <- paste('logR.E ~ s(W.E, k=',knot.num,', bs="bs", m=1)')
m_ald <- evgam(as.formula(fmla_ald), data=as.data.frame(cbind(logR.E,W.E)), family="ald", ald.args=list(tau=0.5), knots=knots1)
thresh.smooth1 <- exp(predict(m_ald, newdata=list(W.E=W.E))$location) # Estimates at all observed W.E values (for threshold-exceedance calculations)
thresh.smooth1a <- exp(predict(m_ald, newdata=list(W.E=Wstar))$location) # Estimates at all observed W* values only
fmla_ald <- paste('logR.E ~ s(W.E, k=',knot.num+1,', bs="bs", m=2)')
m_ald <- evgam(as.formula(fmla_ald), data=as.data.frame(cbind(logR.E,W.E)), family="ald", ald.args=list(tau=0.5), knots=knots2)
thresh.smooth2 <- exp(predict(m_ald, newdata=list(W.E=W.E))$location)
thresh.smooth2a <- exp(predict(m_ald, newdata=list(W.E=Wstar))$location)
fmla_ald <- paste('logR.E ~ s(W.E, k=',knot.num+2,', bs="bs", m=3)')
m_ald <- evgam(as.formula(fmla_ald), data=as.data.frame(cbind(logR.E,W.E)), family="ald", ald.args=list(tau=0.5), knots=knots3)
thresh.smooth3 <- exp(predict(m_ald, newdata=list(W.E=W.E))$location)
thresh.smooth3a <- exp(predict(m_ald, newdata=list(W.E=Wstar))$location)
# Extract threshold exceedances for subsequent GPD fit:
exceeds1 <- apply(cbind(R.E,thresh.smooth1),1,function(x){x[1]>x[2]})
exceeds2 <- apply(cbind(R.E,thresh.smooth2),1,function(x){x[1]>x[2]})
exceeds3 <- apply(cbind(R.E,thresh.smooth3),1,function(x){x[1]>x[2]})
R.matrix.pred <- as.data.frame(cbind(Wstar, rep(NA,length(Wstar))))
names(R.matrix.pred) <- c("W.E","R.E")
# Fit the GPD-GAM (with a spline in the scale parameter and constant shape parameter)
# to the radial components, using the W* values as a covariate in the spline.
# Calculate the required radial quantile for each W*.
# Do this for splines of degree 1,2,3 and select the best by comparing to the individual GPD estimates:
R.matrix <- as.data.frame(cbind(W.E[exceeds1==T], (R.E[exceeds1==T]-thresh.smooth1[exceeds1==T])))
names(R.matrix) <- c("W.E","R.E")
spl=paste('R.E ~ s(W.E, k=',knot.num,', bs="bs", m=1)')
fmla_gpd <- list(as.formula(spl), ~1)
m_gpd <- evgam(fmla_gpd, data=R.matrix, family="gpd", knots=knots1)
gpd_pred <- predict(m_gpd, newdata=R.matrix.pred, prob=(quant-thresh.quant)/(1-thresh.quant))
R.W.quant1 <- thresh.smooth1a + gpd_pred$`q`
errors <- sum(abs(R.W.quant1 - R.W.quant.ind))
R.matrix <- as.data.frame(cbind(W.E[exceeds2==T], (R.E[exceeds2==T]-thresh.smooth2[exceeds2==T])))
names(R.matrix) <- c("W.E","R.E")
spl=paste('R.E ~ s(W.E, k=',knot.num+1,', bs="bs", m=2)')
fmla_gpd <- list(as.formula(spl), ~1)
m_gpd <- evgam(fmla_gpd, data=R.matrix, family="gpd", knots=knots2)
gpd_pred <- predict(m_gpd, newdata=R.matrix.pred, prob=(quant-thresh.quant)/(1-thresh.quant))
R.W.quant2 <- thresh.smooth2a + gpd_pred$`q`
errors[2] <- sum(abs(R.W.quant2 - R.W.quant.ind))
R.matrix <- as.data.frame(cbind(W.E[exceeds3==T], (R.E[exceeds3==T]-thresh.smooth3[exceeds3==T])))
names(R.matrix) <- c("W.E","R.E")
spl=paste('R.E ~ s(W.E, k=',knot.num+2,', bs="bs", m=3)')
fmla_gpd <- list(as.formula(spl), ~1)
m_gpd <- evgam(fmla_gpd, data=R.matrix, family="gpd", knots=knots3)
gpd_pred <- predict(m_gpd, newdata=R.matrix.pred, prob=(quant-thresh.quant)/(1-thresh.quant))
R.W.quant3 <- thresh.smooth3a + gpd_pred$`q`
errors[3] <- sum(abs(R.W.quant3 - R.W.quant.ind))
# Select R-quantile estimates using spline degree with smallest error
degree <- which.min(errors)
R.W.quant <- R.W.quant1*(degree==1) + R.W.quant2*(degree==2) + R.W.quant3*(degree==3)
# Transform back to original coordinates to obtain boundary estimates:
# Only keep estimates **within** range of observed W values:
boundary <- cbind(R.W.quant*Wstar, R.W.quant*(1-Wstar))
boundary <- boundary[-c(1,nrow(boundary)),]
# Scale the boundary estimates using Hill estimate of eta then truncating/stretching:
etaHat <- eta.hill.exp(data)
xstar <- max(apply(boundary,1,min))
bound2 <- etaHat*boundary/xstar
bound2[bound2[,1]>1,1] <- 1
bound2[bound2[,2]>1,2] <- 1
bound3 <- bound2
bound3 <- apply(bound3, 2, function(x){x/max(x)})
bound.scaled <- bound3
# Calculate estimates of alpha using N+W (2020) result:
alpha1 <- max(bound.scaled[bound.scaled[,1]==1,2])
alpha2 <- max(bound.scaled[bound.scaled[,2]==1,1])
# Calculate estimates of lambda(omega) at specified values of omega using N+W (2020) result:
lambda <- NULL
for(iter in 1:length(omegas)){
w <- omegas[iter]
lambda[iter] <- 1/max(apply(bound.scaled,1,function(x){min((x[1]/w),(x[2]/(1-w)))}))
}
return(list(boundary=boundary, boundary.scaled=bound.scaled, lambda=lambda, deg=degree, Wrange=range(W.E)))
}
eta.hill.exp <- function(data, quant=0.95){
Q <- apply(data,1,min)
u <- quantile(Q,quant)
Q.ext <- Q[Q>u]
eta.est <- min(1, mean(Q.ext-u))
return(eta.est)
}
theoretical_properties = function(est,w){
#Imposing lower bound
est[est<pmax(w,1-w)] = pmax(w,1-w)[est<pmax(w,1-w)]
est[1] = 1
est[length(w)] = 1
#Imposing the shape constraints on lambda
for(j in floor(length(w)/2):1){
if((w/est)[j+1]<(w/est)[j] ){
est[j] = (w[j])*(est/w)[j+1]
}
if(((1-w)/est)[j+1]>((1-w)/est)[j] ){
est[j] = ((1-w)[j])*(est/(1-w))[j+1]
}
}
for(j in (floor(length(w)/2)+2):(length(w))){
if((w/est)[j]<(w/est)[j-1] ){
est[j] = (w[j])*(est/w)[j-1]
}
if(((1-w)/est)[j]>((1-w)/est)[j-1] ){
est[j] = ((1-w)[j])*(est/(1-w))[j-1]
}
}
return(est)
}
est_lam_wrapper = function(data){
est_lam_hill = c()
r = c()
for(i in 1:length(w)){
min_proj = pmin(data[,1]/w[i],data[,2]/(1-w[i]))
est_lam_hill[i] = 1/mean(min_proj[min_proj>quantile(min_proj,0.90)] - quantile(min_proj,0.90)) #hill estimator of the angular dependence function
r[((i-1)*m+1):(i*m)] = as.vector(quantile(min_proj,q2) - quantile(min_proj,q1))
}
est_beta_cl = minimisation_function(w_mat=w_mat,k=k_cl,q=q,data_exp = data)
basis_cl = bp2d(w=w_mat,k=k_cl)
est_lam_cl = as.vector(basis_cl%*%est_beta_cl)
est_beta_pr = minimisation_function2(w_mat = w_mat,k=k_pr,frac = frac,r=r,no_quant = m)
basis_pr = bp2d(w=w_mat,k=k_pr)
est_lam_pr = as.vector(basis_pr%*%est_beta_pr)
alphas = heff_tawn_alphas(data = data,q=0.9)
est_lam_hill2 = c()
est_lam_cl2 = c()
est_lam_pr2 = c()
alpha_index = w < (alphas[1]/(1+alphas[1])) | w > (1/(1+alphas[2]))
if(sum(!alpha_index) < 2){
est_lam_hill2 = pmax(w,1-w)
est_lam_cl2 = pmax(w,1-w)
est_lam_pr2 = pmax(w,1-w)
} else {
est_lam_hill2[alpha_index] = pmax(w,1-w)[alpha_index]
est_lam_cl2[alpha_index] = pmax(w,1-w)[alpha_index]
est_lam_pr2[alpha_index] = pmax(w,1-w)[alpha_index]
est_lam_hill2[!alpha_index] = est_lam_hill[!alpha_index]
a = (alphas[1]/(1+alphas[1]))
b = (1/(1+alphas[2]))
v = seq(a,b,length.out = sum(!alpha_index))
frac2 = rep((1-q2)/(1-q1),length(v))
r2 = c()
for(i in 1:length(v)){
min_proj = pmin(data[,1]/(v[i]),data[,2]/(1-v[i]))
for(j in 1:length(q1)){
r2[(i-1)*length(q1) + j] = quantile(min_proj,q2[j]) - quantile(min_proj,q1[j])
}
}
est_beta_cl2 = minimisation_function3(v=v,k=k_cl,q=q,data_exp = data,lam_end=c((1/(1+alphas[1])),(1/(1+alphas[2]))))
basis2_cl = bp2d_const(v=v,k=k_cl,a=a,b=b)
est_lam_cl2[!alpha_index] = basis2_cl%*%est_beta_cl2
est_beta_pr2 = minimisation_function4(v=v,k=k_pr,frac=frac2,r=r2,no_quant=length(q1),lam_end=c((1/(1+alphas[1])),(1/(1+alphas[2]))))
basis2_pr = bp2d_const(v=v,k=k_pr,a=a,b=b)
est_lam_pr2[!alpha_index] = basis2_pr%*%est_beta_pr2
}
estimate_st = consistentEstimation(data,  quant=0.999, thresh.quant=0.5, nei=100, len=199, knot.num=7, deltas=seq(0.01,1,0.01), omegas=w)
est_lam_st = estimate_st$lambda
return(rbind(est_lam_hill,est_lam_cl,est_lam_pr,est_lam_hill2,est_lam_cl2,est_lam_pr2,est_lam_st))
}
}
w = seq(0,1,length.out=1001)#rays considered for estimation
w_mat = cbind(w,1-w)
k_cl = 7#BB polynomial degree
k_pr = 7#BB polynomial degree
q = 0.9#quantile level for min-projection
q1 = seq(0.87,0.93,by=0.002) #quantile levels considered for probability ratio estimators
q2 = q1 + 0.05
frac = rep((1-q2)/(1-q1),length(w))
m=length(q1)
lapply(exp_datasets, summary)
?evd::fbvevd
?evd:::fbvevd
evd:::fbvevd
?bvdist
?evd::bvdist
lambda_estimates = list()
for(cn in 1:dim(unique_pairs)[1]){
lambda_estimates[[cn]] = est_lam_wrapper(data=cbind(exp_datasets[[unique_pairs[cn,1]]],exp_datasets[[unique_pairs[cn,2]]]))
}
unique_pairs = cbind(c(1,1,1,1,1),c(2,3,4,5,6))
lambda_estimates = list()
for(cn in 1:dim(unique_pairs)[1]){
lambda_estimates[[cn]] = est_lam_wrapper(data=cbind(exp_datasets[[unique_pairs[cn,1]]],exp_datasets[[unique_pairs[cn,2]]]))
}
lambda_estimates = lapply(lambda_estimates,function(x){x[1:6,] = t(apply(x[1:6,],1,theoretical_properties,w=w));return(x)})
lambda_estimates = lapply(lambda_estimates,function(x){x[c(1:2, 4:5),] = t(apply(x[c(1:2, 4:5),],1,theoretical_properties,w=w));return(x)})
colours = brewer.pal(7, "Dark2")
lambda_estimates[[1]]
lambda_estimates[[1]][1, ]
lambda_estimates[[1]][2, ]
lambda_estimates[[1]][3, ]
lambda_estimates[[1]][5, ]
lambda_estimates[[1]][6, ]
lambda_estimates[[1]][7, ]
lambda_estimates[[1]][8, ]
lambda_estimates = list()
for(cn in 1:dim(unique_pairs)[1]){
lambda_estimates[[cn]] = est_lam_wrapper(data=cbind(exp_datasets[[unique_pairs[cn,1]]],exp_datasets[[unique_pairs[cn,2]]]))
}
pdf(file="adf_ests_sub_lidia.pdf",width=9,height=6)
par(mfrow=c(2,3),mgp=c(2.5,1,0),mar=c(5,4,4,2)+0.1)
for(cn in 1:dim(unique_pairs)[1]){
plot(w,lambda_estimates[[cn]][1,] ,type="l",col=colours[3],lwd=2,ylim=c(0.5,1.2),xlab=expression(w),ylab=expression(lambda(w)),main = paste0(riv_names[cn+1]," vs Lune - original margins"),cex.lab=1.2, cex.axis=1.2,cex.main=1.4)
lines(w,lambda_estimates[[cn]][2,],lwd=2,col=colours[4],lty=1)
lines(w,lambda_estimates[[cn]][4,],lwd=2,col=colours[5],lty=1)
lines(w,lambda_estimates[[cn]][5,],lwd=2,col=colours[6],lty=1)
lines(w,pmax(w,1-w),lwd=2,col=1,lty=2)
}
plot(1, type="n", axes=FALSE, xlab="", ylab="")
legend(.75,1.35,cex=1, legend =c('Lower Bound', expression(paste(lambda[H])),expression(paste(lambda[H2])), expression(paste(lambda[CL])), expression(paste(lambda[CL2]))), lwd=2, lty=c(2,rep(1,3)),col = c(1, colours[3:6]))
dev.off()
prob
ReturnCurves:::minfunction_mle
ReturnCurves:::est_beta
ReturnCurves:::HeffTawnNegLL
?ismev::gpd.fit
data <- cbind(rnorm(1000), rnorm(1000))
plot(data, pch = 20))
plot(data, pch = 20)
par(mfrow = c(1,1))
plot(data, pch = 20)
dataexp <- margtransf(data)
library(ReturnCurves)
dataexp <- margtransf(data)
plot(dataexp, pch = 20)
hist(dataexp[, 1])
hist(dataexp[, 2])
data <- cbind(rnorm(1000), rnorm(1000))
dataexp <- margtransf(data)
plot(dataexp, pch = 20)
dim(data)
prob <- 10/(dim(data)[1])
rc <- rc_est(data = dataexp, p = prob, method = "hill")
rc_orig <- curvetransf(curvedata = rc, data = data)
unc <- rc_unc(data = data, p = prob, method = "hill")
plot(data, xlab = "X", ylab = "Y", pch = 16)
lines(rc_orig, lwd = 2, col = 2)
lines(unc$median, lwd = 2, col = "orange") # to plot median estimates
lines(unc$mean, lwd = 2, col = "orange")
lines(unc$upper, lty = 'dashed', col = 'blue', lwd = 2)
lines(unc$lower, lty = 'dashed', col = 'blue', lwd = 2)
plot(data, xlab = "X", ylab = "Y", pch = 16, col = "grey")
lines(rc_orig, lwd = 2, col = 2)
lines(unc$median, lwd = 2, col = "orange") # to plot median estimates
lines(unc$mean, lwd = 2, col = "blue")
lines(unc$lower, lty = 'dashed', lwd = 2)
lines(unc$upper, lty = 'dashed', lwd = 2)
rc_orig
gof <- rc_gof(data = data, rc_origin = rc_orig)
ang <- 1:length(gof$median)
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability")
gof$median
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability", type = "n")
polygon(c(rev(ang), ang), c(rev(gof$lower), gof$upper), col = 'grey80', border = NA)
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability", type = "n",
#' ylim = c(-0.001, range(gof$upper)[2] + 0.001))
)
polygon(c(rev(ang), ang), c(rev(gof$lower), gof$upper), col = 'grey80', border = NA)
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability", type = "n",
#' ylim = c(-0.001, range(gof$upper)[2] + 0.001)))
)
ang <- 1:length(gof$median)
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability", type = "n", ylim = c(-0.001, range(gof$upper)[2] + 0.001))
polygon(c(rev(ang), ang), c(rev(gof$lower), gof$upper), col = 'grey80', border = NA)
lines(ang, gof$median, lwd = 2)
lines(ang, gof$upper, lty = 'dashed', col = 'blue', lwd = 2)
lines(ang, gof$lower, lty = 'dashed', col = 'blue', lwd = 2)
lines(ang, rep(prob, length(ang)), lwd = 3, col = 2)
1/1000
10/1000
library(ReturnCurves)
?margtransf
example("margtransf")
rm(list = ls())
example("margtransf")
dataexp
plot(dataexp, pch = 20)
?curvetransf
example("curvetransf")
plot(data, pch = 20, main = "Return Curve on the original margins")
lines(rc_orig, col = 2, lwd = 2)
?rc_est
example(rc_est)
plot(dataexp, pch = 20, main = "Return Curve on exponential margins")
lines(rc, col = 2, lwd = 2)
plot(dataexp, pch = 20, main = "Return Curve on exponential margins")
lines(rc, col = 2, lwd = 2)
?rc_unt
?rc_unc
example(rc_unc)
plot(data, xlab = "X", ylab = "Y", pch = 20, col = "grey")
lines(rc_orig, lwd = 2, col = 2)
lines(unc$median, lwd = 2, col = "orange") # to plot median estimates
lines(unc$mean, lwd = 2, col = "orange") # to plot mean estimates
lines(unc$upper, lty = 'dashed', lwd = 2)
lines(unc$lower, lty = 'dashed', lwd = 2)
?rc_gof
example(rc_gof)
ang <- 1:length(gof$median)
plot(ang, gof$median, xlab = "Angle Index", ylab = "Probability", type = "n", ylim = c(-0.001, range(gof$upper)[2] + 0.001))
polygon(c(rev(ang), ang), c(rev(gof$lower), gof$upper), col = 'grey80', border = NA)
lines(ang, gof$median, lwd = 2)
lines(ang, gof$upper, lty = 'dashed', col = 'blue', lwd = 2)
lines(ang, gof$lower, lty = 'dashed', col = 'blue', lwd = 2)
lines(ang, rep(prob, length(ang)), lwd = 3, col = 2)
?adf_est
example(adf_est)
plot(w, pmax(w, 1-w), type = "l", lty = 2)
lines(w, adf, col = 2, lwd = 2)
plot(w, pmax(w, 1-w), type = "l", lty = 2, ylim = c(min(pmax(w, 1-w)), max(adf) + 0.1))
plot(w, pmax(w, 1-w), type = "l", lty = 2)
lines(w, adf, col = 2, lwd = 2)
plot(w, pmax(w, 1-w), type = "l", lty = 2, ylim = c(min(pmax(w, 1-w)), max(adf) + 0.1))
lines(w, adf, col = 2, lwd = 2)
plot(w, pmax(w, 1-w), type = "l", lty = 2, ylim = c(min(pmax(w, 1-w)), max(adf) + 0.01))
lines(w, adf, col = 2, lwd = 2)
seq(0, 1, by = 0.001)
?adf_gof
example(adf_gof)
plot(gof$model, gof$empirical, ylab = "Empirical", xlab = "Model")
polygon(c(rev(gof$model), gof$model), c(rev(gof$lower), gof$upper), col = 'grey', border = NA)
points(gof$model, gof$empirical, pch = 20, col = "black")
abline(0, 1, col = 2,lwd = 3)
?adf_est
library(ReturnCurves)
?adf_est
?rc_unc
example(rc_unc)
plot(data, xlab = "X", ylab = "Y", pch = 20, col = "grey")
lines(rc_orig, lwd = 2, col = 2)
lines(unc$median, lwd = 2, col = "orange") # to plot median estimates
lines(unc$mean, lwd = 2, col = "orange") # to plot mean estimates
lines(unc$upper, lty = 'dashed', lwd = 2)
lines(unc$lower, lty = 'dashed', lwd = 2)
?rc_est
?curvetransf
?rc_unc
?rc_gof
?rc_gof
?curvetransf
?rank
?optim
library(ReturnCurves)
?adf_est
library(ReturnCurves)
?adf_est
library(ReturnCurves)
?adf_est
example(adf_est)
?adf_est
plot(w, pmax(w, 1-w), type = "l", lty = 2, ylim = c(min(pmax(w, 1-w)), max(adf) + 0.1))
lines(w, adf, col = 2, lwd = 2)
?rc_est
example(rc_est)
library(ReturnCurves)
?rc_est
example(rc_est)
plot(dataexp, pch = 20, main = "Return Curve on exponential margins")
lines(rc, col = 2, lwd = 2
)
example("curvetransf")
?curve
?curvetransf
plot(data, pch = 20, main = "Return Curve on the original margins")
lines(rc_orig, col = 2, lwd = 2)
example(adf_gof)
example(rc_unc)
example("rc_gof")
example(adf_gof)
?adf-gof
?adf_gof
library(ReturnCurves)
?adf_gof
example("adf_gof")
setwd("~/luna/Return Curves Project/ReturnCurves/R")
library(ReturnCurves)
?adf_gof
example("adf_gof")
library(ReturnCurves)
?adf_est
library(ReturnCurves)
adf_gof
?adf_gof
library(ReturnCurves)
?adf_gof
library(ReturnCurves)
example(adf_est)
example(adf_gof)
?curvetransf
example("curvetransf")
library(ReturnCurves)
?margtransf
example("margtransf")
library(ReturnCurves)
?rc_est
library(ReturnCurves)
?rc_est
library(ReturnCurves)
?rc_est
library(ReturnCurves)
?rc_est
example(rc_est)
plot(dataexp, pch = 20, main = "Return Curve on exponential margins")
lines(rc, col = 2, lwd = 2)
example("curvetransf")
library(ReturnCurves)
?rc_gof
example(rc_gof)
library(ReturnCurves)
example(rc_gof)
?rc_gof
library(ReturnCurves)
?rc_unc
example(rc_unc)
?rc_est
?rc
library(ReturnCurves)
?rc_o
example(rc_o)
rc_o(data = data, p = prob, method="hill")
